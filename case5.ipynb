{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b13fb10",
   "metadata": {},
   "source": [
    "1 imagenet256のラベル付けを変更する \n",
    "\n",
    "2 imagenet_class_index_lower.jsonにimagenet-256のデータセットのラベルと会うように文字列を修正（手動＋コード）\n",
    "\n",
    "3 事前学習済みのresnet50を用いてimagenet-256のデータセットの画像分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943c948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ffaee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gotou/miniconda3/envs/env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gotou/miniconda3/envs/env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 1. 前処理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# 2. データセット\n",
    "data_dir = \"./imagenet-256\"\n",
    "val_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# 3. モデル\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49923633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] マッチできたクラス数: 999/1000\n",
      "[WARN] マッチできなかったクラス例: ['tank_suit'] ...\n"
     ]
    }
   ],
   "source": [
    "# 4. ImageNet クラスインデックスを読み込み\n",
    "with open(\"imagenet_class_index_lower.json\", \"r\") as f:\n",
    "    class_idx = json.load(f)\n",
    "\n",
    "# name → index の辞書を作成（空白はアンダースコアに変換）\n",
    "name_to_idx = {}\n",
    "for k, v in class_idx.items():\n",
    "    idx = int(k)\n",
    "    synset, name = v\n",
    "    # \"afghan hound\" → \"afghan_hound\"\n",
    "    norm_name = name.replace(\" \", \"_\")\n",
    "    name_to_idx[norm_name] = idx\n",
    "\n",
    "# abc順 (val_dataset.classes) → ImageNetインデックス\n",
    "abc2idx = {}\n",
    "missing = []\n",
    "for cls_name in val_dataset.classes:\n",
    "    if cls_name in name_to_idx:\n",
    "        abc2idx[cls_name] = name_to_idx[cls_name]\n",
    "    else:\n",
    "        missing.append(cls_name)\n",
    "\n",
    "print(f\"[INFO] マッチできたクラス数: {len(abc2idx)}/{len(val_dataset.classes)}\")\n",
    "if missing:\n",
    "    print(f\"[WARN] マッチできなかったクラス例: {missing[:10]} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf09dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 86/4218 [00:15<12:40,  5.43batch/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m valid_labels_idx \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m valid_paths \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     18\u001b[0m     cls_name \u001b[38;5;241m=\u001b[39m val_dataset\u001b[38;5;241m.\u001b[39mclasses[l]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cls_name \u001b[38;5;129;01min\u001b[39;00m abc2idx:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. 精度計算\n",
    "correct1, correct5, total = 0, 0, 0\n",
    "\n",
    "# ...existing code...\n",
    "\n",
    "correct_top1_paths = []\n",
    "correct_top5_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(val_loader, desc=\"Evaluating\", unit=\"batch\")):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        valid_indices = []\n",
    "        valid_labels_idx = []\n",
    "        valid_paths = []\n",
    "        for i, l in enumerate(labels.cpu()):\n",
    "            cls_name = val_dataset.classes[l]\n",
    "            if cls_name in abc2idx:\n",
    "                valid_indices.append(i)\n",
    "                valid_labels_idx.append(abc2idx[cls_name])\n",
    "                img_idx = batch_idx * val_loader.batch_size + i\n",
    "                img_path, _ = val_dataset.samples[img_idx]\n",
    "                valid_paths.append(img_path)\n",
    "\n",
    "        if not valid_indices:\n",
    "            continue\n",
    "\n",
    "        valid_images = images[valid_indices]\n",
    "        valid_labels_idx = torch.tensor(valid_labels_idx).to(device)\n",
    "        valid_outputs = outputs[valid_indices]\n",
    "\n",
    "        # top-1\n",
    "        _, pred1 = valid_outputs.max(1)\n",
    "        correct_mask1 = pred1.eq(valid_labels_idx)\n",
    "        for idx, is_correct in enumerate(correct_mask1.cpu()):\n",
    "            if is_correct:\n",
    "                correct_top1_paths.append(valid_paths[idx])\n",
    "\n",
    "        # top-5\n",
    "        _, pred5 = valid_outputs.topk(5, 1, True, True)\n",
    "        correct_mask5 = pred5.eq(valid_labels_idx.view(-1,1)).any(dim=1)\n",
    "        for idx, is_correct in enumerate(correct_mask5.cpu()):\n",
    "            if is_correct:\n",
    "                correct_top5_paths.append(valid_paths[idx])\n",
    "\n",
    "        correct1 += correct_mask1.sum().item()\n",
    "        correct5 += correct_mask5.sum().item()\n",
    "        total += len(valid_indices)\n",
    "\n",
    "print(f\"Top-1 Accuracy: {100 * correct1 / total:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {100 * correct5 / total:.2f}%\")\n",
    "\n",
    "# 正解画像パスをファイルに保存\n",
    "with open(\"correct_top1_images.txt\", \"w\") as f:\n",
    "    for path in correct_top1_paths:\n",
    "        f.write(path + \"\\n\")\n",
    "\n",
    "with open(\"correct_top5_images.txt\", \"w\") as f:\n",
    "    for path in correct_top5_paths:\n",
    "        f.write(path + \"\\n\")\n",
    "\n",
    "print(f\"Top-1で正解した画像数: {len(correct_top1_paths)}\")\n",
    "print(f\"Top-5で正解した画像数: {len(correct_top5_paths)}\")\n",
    "# ...existing code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a7177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FGSM Attack (batch):   3%|▎         | 449/14412 [01:31<47:38,  4.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m batch_paths:\n\u001b[1;32m     20\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(input_tensor)\n\u001b[1;32m     23\u001b[0m     cls_name \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1769\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1766\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1770\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1771\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32  # メモリに合わせて調整\n",
    "epsilon = 0.03\n",
    "\n",
    "# 正解画像パスの読み込み\n",
    "with open(\"correct_top1_images.txt\") as f:\n",
    "    correct_paths = [line.strip() for line in f]\n",
    "\n",
    "adv_correct1, adv_correct5, adv_total = 0, 0, 0\n",
    "\n",
    "model.eval()\n",
    "for i in tqdm(range(0, len(correct_paths), batch_size), desc=\"FGSM Attack (batch)\"):\n",
    "    batch_paths = correct_paths[i:i+batch_size]\n",
    "    images = []\n",
    "    labels = []\n",
    "    for path in batch_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        input_tensor = transform(img)\n",
    "        images.append(input_tensor)\n",
    "        cls_name = path.split(\"/\")[-2]\n",
    "        if cls_name in abc2idx:\n",
    "            labels.append(abc2idx[cls_name])\n",
    "        else:\n",
    "            labels.append(-1)  # 無効値\n",
    "\n",
    "    # 無効なラベルを除外\n",
    "    valid_indices = [j for j, l in enumerate(labels) if l >= 0]\n",
    "    if not valid_indices:\n",
    "        continue\n",
    "    images = torch.stack([images[j] for j in valid_indices]).to(device)\n",
    "    labels = torch.tensor([labels[j] for j in valid_indices]).to(device)\n",
    "    images.requires_grad = True\n",
    "\n",
    "    # FGSM\n",
    "    output = model(images)\n",
    "    loss = torch.nn.functional.cross_entropy(output, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = images.grad.data\n",
    "    perturbed = images + epsilon * data_grad.sign()\n",
    "    perturbed = torch.clamp(perturbed, 0, 1)\n",
    "\n",
    "    # 再分類\n",
    "    output_adv = model(perturbed)\n",
    "    _, pred1 = output_adv.max(1)\n",
    "    _, pred5 = output_adv.topk(5, 1, True, True)\n",
    "\n",
    "    adv_total += len(labels)\n",
    "    adv_correct1 += pred1.eq(labels).sum().item()\n",
    "    adv_correct5 += sum([labels[j].item() in pred5[j].cpu().numpy() for j in range(len(labels))])\n",
    "\n",
    "print(f\"[FGSM] Top-1 Accuracy: {100 * adv_correct1 / adv_total:.2f}%\")\n",
    "print(f\"[FGSM] Top-5 Accuracy: {100 * adv_correct5 / adv_total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a664388",
   "metadata": {},
   "source": [
    "数枚だけ敵対的画像を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcdf8416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FGSM Attack (100 samples): 100%|██████████| 4/4 [00:00<00:00,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLEAN] Total: 100\n",
      "[CLEAN] Top-1: 100/100 (100.00%)\n",
      "[CLEAN] Top-5: 100/100 (100.00%)\n",
      "[FGSM] Total: 100\n",
      "[FGSM] Top-1: 26/100 (26.00%)\n",
      "[FGSM] Top-5: 47/100 (47.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 32\n",
    "epsilon = 0.03\n",
    "\n",
    "# 正解画像パス（先頭100枚のみ）\n",
    "with open(\"correct_top1_images.txt\") as f:\n",
    "    correct_paths = [line.strip() for line in f][:100]\n",
    "\n",
    "# 精度カウント用\n",
    "clean_correct1, clean_correct5 = 0, 0\n",
    "adv_correct1, adv_correct5 = 0, 0\n",
    "total = 0\n",
    "\n",
    "model.eval()\n",
    "for i in tqdm(range(0, len(correct_paths), batch_size), desc=\"FGSM Attack (100 samples)\"):\n",
    "    batch_paths = correct_paths[i:i+batch_size]\n",
    "    images, labels = [], []\n",
    "    for path in batch_paths:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        input_tensor = transform(img)\n",
    "        images.append(input_tensor)\n",
    "        cls_name = path.split(\"/\")[-2]\n",
    "        labels.append(abc2idx.get(cls_name, -1))\n",
    "\n",
    "    # 無効ラベルを除外\n",
    "    valid_indices = [j for j, l in enumerate(labels) if l >= 0]\n",
    "    if not valid_indices:\n",
    "        continue\n",
    "    images = torch.stack([images[j] for j in valid_indices]).to(device)\n",
    "    labels = torch.tensor([labels[j] for j in valid_indices]).to(device)\n",
    "    images.requires_grad = True\n",
    "\n",
    "    # ====== クリーン画像で推論 ======\n",
    "    output_clean = model(images)\n",
    "    _, pred1_clean = output_clean.max(1)\n",
    "    _, pred5_clean = output_clean.topk(5, 1, True, True)\n",
    "\n",
    "    clean_correct1 += pred1_clean.eq(labels).sum().item()\n",
    "    clean_correct5 += sum([labels[j].item() in pred5_clean[j].cpu().numpy()\n",
    "                           for j in range(len(labels))])\n",
    "\n",
    "    # ====== FGSM 攻撃 ======\n",
    "    loss = F.cross_entropy(output_clean, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    data_grad = images.grad.data\n",
    "    perturbed = torch.clamp(images + epsilon * data_grad.sign(), 0, 1)\n",
    "\n",
    "    # 再分類（敵対的画像）\n",
    "    output_adv = model(perturbed)\n",
    "    _, pred1_adv = output_adv.max(1)\n",
    "    _, pred5_adv = output_adv.topk(5, 1, True, True)\n",
    "\n",
    "    adv_correct1 += pred1_adv.eq(labels).sum().item()\n",
    "    adv_correct5 += sum([labels[j].item() in pred5_adv[j].cpu().numpy()\n",
    "                         for j in range(len(labels))])\n",
    "\n",
    "    total += len(labels)\n",
    "\n",
    "# ====== 結果表示 ======\n",
    "print(f\"[CLEAN] Total: {total}\")\n",
    "print(f\"[CLEAN] Top-1: {clean_correct1}/{total} ({100 * clean_correct1 / total:.2f}%)\")\n",
    "print(f\"[CLEAN] Top-5: {clean_correct5}/{total} ({100 * clean_correct5 / total:.2f}%)\")\n",
    "\n",
    "print(f\"[FGSM] Total: {total}\")\n",
    "print(f\"[FGSM] Top-1: {adv_correct1}/{total} ({100 * adv_correct1 / total:.2f}%)\")\n",
    "print(f\"[FGSM] Top-5: {adv_correct5}/{total} ({100 * adv_correct5 / total:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65f29473",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'guided_diffusion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# --- guided-diffusion imports (make sure repo is available in PYTHONPATH) ---\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mguided_diffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscript_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model_and_diffusion, model_and_diffusion_defaults\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ---------------- user settings ----------------\u001b[39;00m\n\u001b[1;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'guided_diffusion'"
     ]
    }
   ],
   "source": [
    "# fgsm_plus_diffusion_eval.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import math\n",
    "\n",
    "# --- guided-diffusion imports (make sure repo is available in PYTHONPATH) ---\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "\n",
    "# ---------------- user settings ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16   # VRAM に合わせて調整。256x256 diffusion は重いので小さめ推奨\n",
    "epsilon = 0.03\n",
    "num_samples = 100  # correct_top1_images.txt の先頭何枚を使うか\n",
    "\n",
    "# Diffusion settings\n",
    "use_ddim = True\n",
    "ddim_steps = 50     # 速くしたければ 25~50。品質を優先するなら 100+\n",
    "real_step = 250     # reverse の深さ（実験して調整）\n",
    "model_path = \"models/256x256_diffusion_uncond.pt\"  # 事前チェックポイント\n",
    "\n",
    "# ---------------- transforms (あなたの分類用) ----------------\n",
    "# 既存の分類 transform と同じものを使う想定\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "# ImageNet 正規化パラ\n",
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(3,1,1)\n",
    "imagenet_std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(3,1,1)\n",
    "\n",
    "# ---------------- load classifier (あなたの既存モデル) ----------------\n",
    "# ここは既にロード済みの model を使っているなら置き換えてください\n",
    "clf = models.resnet50(pretrained=True).to(device)\n",
    "clf.eval()\n",
    "\n",
    "# ---------------- load paths ----------------\n",
    "with open(\"correct_top1_images.txt\") as f:\n",
    "    all_paths = [p.strip() for p in f]\n",
    "paths = all_paths[:num_samples]\n",
    "\n",
    "# ---------------- guided-diffusion model load ----------------\n",
    "defaults = model_and_diffusion_defaults()\n",
    "defaults.update({\n",
    "    'image_size': 256,\n",
    "    'num_channels': 256,\n",
    "    'num_head_channels': 64,\n",
    "    'num_res_blocks': 2,\n",
    "    'learn_sigma': True,\n",
    "    'class_cond': False,\n",
    "    'use_fp16': False,\n",
    "    'use_scale_shift_norm': True,\n",
    "    'noise_schedule': 'linear',\n",
    "    'diffusion_steps': 1000,\n",
    "})\n",
    "print(\"Creating guided-diffusion model...\")\n",
    "diff_model, diffusion = create_model_and_diffusion(**defaults)\n",
    "# load checkpoint\n",
    "print(\"Loading checkpoint:\", model_path)\n",
    "ckpt = torch.load(model_path, map_location='cpu')\n",
    "diff_model.load_state_dict(ckpt)\n",
    "diff_model.to(device)\n",
    "diff_model.eval()\n",
    "\n",
    "# Utility: unnormalize (ImageNet norm -> [0,1])\n",
    "def unnormalize(x_norm):\n",
    "    # x_norm: (B,3,H,W), torch tensor on device\n",
    "    return x_norm * imagenet_std + imagenet_mean  # still on device\n",
    "\n",
    "# Utility: prepare tensor for diffusion: [0,1] -> [-1,1], resize to 256\n",
    "def prepare_for_diffusion(x_01):\n",
    "    # x_01: (B,3,H,W) in [0,1]\n",
    "    # Resize to 256x256 if needed\n",
    "    if x_01.shape[2] != 256 or x_01.shape[3] != 256:\n",
    "        x_01 = F.interpolate(x_01, size=(256,256), mode='bilinear', align_corners=False)\n",
    "    x_neg1_1 = x_01 * 2.0 - 1.0\n",
    "    return x_neg1_1\n",
    "\n",
    "# Utility: after diffusion -> classifier input ([-1,1] -> [0,1] -> resize 224 -> normalize)\n",
    "def diffusion_to_clf_input(x_neg1_1):\n",
    "    x_01 = (x_neg1_1 + 1.0) / 2.0\n",
    "    if x_01.shape[2] != 224 or x_01.shape[3] != 224:\n",
    "        x_01 = F.interpolate(x_01, size=(224,224), mode='bilinear', align_corners=False)\n",
    "    # normalize\n",
    "    x_norm = (x_01 - imagenet_mean) / imagenet_std\n",
    "    return x_norm\n",
    "\n",
    "# Purify function using diffusion (DDIM if specified)\n",
    "@torch.no_grad()\n",
    "def purify_with_diffusion(x_neg1_1, diffusion, diff_model, device, use_ddim=True, ddim_steps=50, real_step=250):\n",
    "    \"\"\"\n",
    "    x_neg1_1: (B,3,256,256) in [-1,1], float32 on device\n",
    "    returns: purified (B,3,256,256) in [-1,1]\n",
    "    \"\"\"\n",
    "    B, C, H, W = x_neg1_1.shape\n",
    "    model_kwargs = {}\n",
    "    # choose reverse + sample functions depending on repo API\n",
    "    # guided-diffusion provides ddim_reverse_sample_loop and ddim_sample_loop\n",
    "    if use_ddim:\n",
    "        # reverse: image -> latent (noise)\n",
    "        latent = diffusion.ddim_reverse_sample_loop(\n",
    "            diff_model,\n",
    "            (B, C, H, W),\n",
    "            noise=x_neg1_1,\n",
    "            model_kwargs=model_kwargs,\n",
    "            real_step=real_step,\n",
    "            clip_denoised=True,\n",
    "            ddim_steps=ddim_steps\n",
    "        )\n",
    "        # sample: latent -> recon\n",
    "        recon = diffusion.ddim_sample_loop(\n",
    "            diff_model,\n",
    "            (B, C, H, W),\n",
    "            noise=latent,\n",
    "            model_kwargs=model_kwargs,\n",
    "            real_step=real_step,\n",
    "            clip_denoised=True,\n",
    "            ddim_steps=ddim_steps\n",
    "        )\n",
    "    else:\n",
    "        # DDPM style (slower) - use p_sample_loop and p_sample_reverse if available\n",
    "        latent = diffusion.p_sample_reverse_loop(\n",
    "            diff_model,\n",
    "            (B, C, H, W),\n",
    "            noise=x_neg1_1,\n",
    "            model_kwargs=model_kwargs,\n",
    "            real_step=real_step,\n",
    "            clip_denoised=True\n",
    "        )\n",
    "        recon = diffusion.p_sample_loop(\n",
    "            diff_model,\n",
    "            (B, C, H, W),\n",
    "            noise=latent,\n",
    "            model_kwargs=model_kwargs,\n",
    "            real_step=real_step,\n",
    "            clip_denoised=True\n",
    "        )\n",
    "    return recon\n",
    "\n",
    "# ---------------- evaluation loop ----------------\n",
    "clean_correct1 = clean_correct5 = adv_correct1 = adv_correct5 = pur_correct1 = pur_correct5 = 0\n",
    "total = 0\n",
    "\n",
    "for i in tqdm(range(0, len(paths), batch_size), desc=\"Process batches\"):\n",
    "    batch_paths = paths[i:i+batch_size]\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for p in batch_paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        t = transform(img)  # normalized, 224x224\n",
    "        imgs.append(t)\n",
    "        cls = p.split(\"/\")[-2]\n",
    "        labels.append(abc2idx.get(cls, -1))\n",
    "    valid_idx = [j for j,l in enumerate(labels) if l>=0]\n",
    "    if not valid_idx:\n",
    "        continue\n",
    "    imgs = torch.stack([imgs[j] for j in valid_idx]).to(device)  # normalized\n",
    "    labels = torch.tensor([labels[j] for j in valid_idx], device=device)\n",
    "    B = imgs.shape[0]\n",
    "    imgs.requires_grad = True\n",
    "\n",
    "    # ---- Clean predictions ----\n",
    "    out_clean = clf(imgs)\n",
    "    _, p1c = out_clean.max(1)\n",
    "    _, p5c = out_clean.topk(5,1,True,True)\n",
    "    clean_correct1 += p1c.eq(labels).sum().item()\n",
    "    clean_correct5 += sum([labels[j].item() in p5c[j].cpu().numpy() for j in range(B)])\n",
    "\n",
    "    # ---- FGSM attack ----\n",
    "    loss = F.cross_entropy(out_clean, labels)\n",
    "    clf.zero_grad()\n",
    "    loss.backward()\n",
    "    grad = imgs.grad.data\n",
    "    perturbed = torch.clamp(imgs + epsilon * grad.sign(), 0, 1)  # still normalized scale? careful below\n",
    "\n",
    "    # NOTE: imgs are normalized; the perturbation was computed in normalized space.\n",
    "    # To pass to diffusion we must unnormalize to [0,1].\n",
    "    unnorm_pert = unnormalize(perturbed)  # now in [0,1] (approximately) -- clamp to be safe\n",
    "    unnorm_pert = torch.clamp(unnorm_pert, 0.0, 1.0)\n",
    "\n",
    "    # ---- FGSM predictions (on normalized perturbed) ----\n",
    "    out_adv = clf(perturbed)\n",
    "    _, p1a = out_adv.max(1)\n",
    "    _, p5a = out_adv.topk(5,1,True,True)\n",
    "    adv_correct1 += p1a.eq(labels).sum().item()\n",
    "    adv_correct5 += sum([labels[j].item() in p5a[j].cpu().numpy() for j in range(B)])\n",
    "\n",
    "    # ---- Purify with diffusion ----\n",
    "    # Prepare for diffusion: resize to 256 and convert to [-1,1]\n",
    "    inp_for_diff = prepare_for_diffusion(unnorm_pert)  # shape (B,3,256,256) in [-1,1]\n",
    "    # run purification\n",
    "    purified_neg1_1 = purify_with_diffusion(inp_for_diff, diffusion, diff_model, device,\n",
    "                                            use_ddim=use_ddim, ddim_steps=ddim_steps, real_step=real_step)\n",
    "    # convert back to classifier input\n",
    "    pur_clf_inp = diffusion_to_clf_input(purified_neg1_1).to(device)\n",
    "\n",
    "    out_pur = clf(pur_clf_inp)\n",
    "    _, p1p = out_pur.max(1)\n",
    "    _, p5p = out_pur.topk(5,1,True,True)\n",
    "    pur_correct1 += p1p.eq(labels).sum().item()\n",
    "    pur_correct5 += sum([labels[j].item() in p5p[j].cpu().numpy() for j in range(B)])\n",
    "\n",
    "    total += B\n",
    "\n",
    "# ---------------- results ----------------\n",
    "print(\"Total:\", total)\n",
    "print(f\"[CLEAN] Top-1: {clean_correct1}/{total} ({100*clean_correct1/total:.2f}%)\")\n",
    "print(f\"[CLEAN] Top-5: {clean_correct5}/{total} ({100*clean_correct5/total:.2f}%)\")\n",
    "print(f\"[FGSM ] Top-1: {adv_correct1}/{total} ({100*adv_correct1/total:.2f}%)\")\n",
    "print(f\"[FGSM ] Top-5: {adv_correct5}/{total} ({100*adv_correct5/total:.2f}%)\")\n",
    "print(f\"[PUR  ] Top-1: {pur_correct1}/{total} ({100*pur_correct1/total:.2f}%)\")\n",
    "print(f\"[PUR  ] Top-5: {pur_correct5}/{total} ({100*pur_correct5/total:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
